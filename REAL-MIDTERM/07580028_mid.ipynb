{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# คำตอบของการสองกลางภาค รายวิชา 517432\n",
    "## เจ้าของ\n",
    "นายภัคพล พงษ์ทวี (07580028)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ข้อ 2.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31 \n",
      " {'isabella', 'indians', 'indiamen', 'instances', 'ishmael', 'ifs', 'italy', 'isolatoes', 'iroquois', 'icelandic', 'imprimis', 'isaiah', 'islanders', 'israel', 'ingin', 'islands', 'illinois', 'indiaman', 'inlanders', 'inserting', 'ireland', 'india', 'ixion', 'isles', 'israelites', 'indies', 'innocents', 'icebergs', 'irish', 'indian', 'italian'}\n"
     ]
    }
   ],
   "source": [
    "#Exam2.2\n",
    "from nltk.book import *\n",
    "from nltk.corpus import words\n",
    "english_vocab = set(words.words())\n",
    "text1_vocab = set([ w for w in text1.vocab() if w.isalpha() and w.istitle() and w[0] == 'I'])\n",
    "result = set([w.lower() for w in text1_vocab]) - english_vocab\n",
    "print(len(result),\"\\n\",result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ข้อ 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categories\t\tNumber of token\t\tNumber of vocabulary\t\tLexical Density\n",
      "adventure \t\t 69342 \t\t 8289 \t\t 8.365544697792254\n",
      "fiction \t\t 68488 \t\t 8680 \t\t 7.890322580645162\n",
      "news \t\t 100554 \t\t 13112 \t\t 7.668852959121415\n",
      "reviews \t\t 40704 \t\t 8069 \t\t 5.044491262857851\n",
      "religion \t\t 39399 \t\t 5931 \t\t 6.642893272635306\n",
      "romance \t\t 70022 \t\t 7883 \t\t 8.882658886210834\n"
     ]
    }
   ],
   "source": [
    "#exam 3\n",
    "from nltk.corpus import brown\n",
    "print('Categories\\t\\tNumber of token\\t\\tNumber of vocabulary\\t\\tLexical Density')\n",
    "for category in ['adventure','fiction','news','reviews','religion','romance']:\n",
    "    token = [w.lower() for w in brown.words(categories = category)]\n",
    "    token_count = len(token)\n",
    "    vocab_count = len(set(token))\n",
    "    print(category,'\\t\\t',token_count,'\\t\\t',vocab_count,'\\t\\t',token_count/vocab_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ข้อ 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7521\n"
     ]
    }
   ],
   "source": [
    "#exam 4\n",
    "from nltk.corpus import words,stopwords,webtext\n",
    "def unknown(text):\n",
    "    english_vocab = set([w.lower() for w in words.words()])\n",
    "    stop_vocab = set([w.lower() for w in stopwords.words('english')])\n",
    "    lower_text = set([w.lower() for w in text if w.isalpha()])\n",
    "    return list(lower_text - english_vocab - stop_vocab)\n",
    "print(len(unknown(webtext.words())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ข้อ 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "[27, 25, 28]\n"
     ]
    }
   ],
   "source": [
    "#exam 5\n",
    "from nltk import FreqDist\n",
    "freq = FreqDist([len(w) for w in unknown(webtext.words())])\n",
    "print(freq.max()) # คำที่ความยากมากสุด\n",
    "print(freq.hapaxes()) #คำที่มีความยามน้อยสุด"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ข้อ 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'fox', 'the', 'dog']\n"
     ]
    }
   ],
   "source": [
    "#Exam8\n",
    "sent = 'The quick brown fox jumps over the lazy dog'.split()\n",
    "result = [w.lower() for w in sent if len(w) == 3]\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ข้อ 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fellow citizens 116\n",
      "the people 260\n",
      "the government 166\n"
     ]
    }
   ],
   "source": [
    "#Exam9\n",
    "from nltk import FreqDist,bigrams\n",
    "from nltk.corpus import inaugural\n",
    "def my_bigram(word,text):\n",
    "    word_pair = list(bigrams([w.lower() for w in text if w.isalpha()]))\n",
    "    interest_pair = [w for w in word_pair if w[1] == word]\n",
    "    freq = FreqDist(interest_pair)\n",
    "    most_pair = freq.most_common(1)[0]\n",
    "    return most_pair[0][0]+\" \"+word+\" \"+str(most_pair[1])\n",
    "print(my_bigram('citizens',inaugural.words()))\n",
    "print(my_bigram('people',inaugural.words()))\n",
    "print(my_bigram('government',inaugural.words()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fellow citizens', 'our citizens', 'its citizens', 'the citizens', 'of citizens']\n",
      "['the people', 'our people', 'american people', 'a people', 'free people']\n",
      "['the government', 'of government', 'our government', 'a government', 'federal government']\n"
     ]
    }
   ],
   "source": [
    "#Exam9 (ต่อ) ให้แสดงผล list ที่พบหน้า สัก 5 อันดับแรก\n",
    "from nltk import FreqDist,bigrams\n",
    "from nltk.corpus import inaugural\n",
    "def my_bigram_list(word,text,size = 5):\n",
    "    word_pair = list(bigrams([w.lower() for w in text if w.isalpha()]))\n",
    "    interest_pair = [w for w in word_pair if w[1] == word]\n",
    "    freq = FreqDist(interest_pair)\n",
    "    output = []\n",
    "    for pair in freq.most_common(size):\n",
    "        output.append(pair[0][0]+\" \"+pair[0][1])    \n",
    "    return output\n",
    "print(my_bigram_list('citizens',inaugural.words()))\n",
    "print(my_bigram_list('people',inaugural.words()))\n",
    "print(my_bigram_list('government',inaugural.words()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ข้อ 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "large aquatic carnivorous mammal with fin-like forelimbs no hind limbs, including: whales; dolphins; porpoises; narwhals\n"
     ]
    }
   ],
   "source": [
    "#Exam 10.1\n",
    "from nltk.corpus import wordnet\n",
    "print(wordnet.synset('cetacean.n.01').definition())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cetacean', 'cetacean_mammal', 'blower']\n"
     ]
    }
   ],
   "source": [
    "#Exam 10.2\n",
    "from nltk.corpus import wordnet\n",
    "print(wordnet.synset('cetacean.n.01').lemma_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.14285714285714285\n",
      "0.1111111111111111\n",
      "0.07692307692307693\n"
     ]
    }
   ],
   "source": [
    "#Exam 10.3\n",
    "mammal = wordnet.synset('mammal.n.01')\n",
    "homo = wordnet.synset('homo.n.01')\n",
    "dolphin = wordnet.synset('dolphin.n.01')\n",
    "print(wordnet.path_similarity(mammal,homo))\n",
    "print(wordnet.path_similarity(mammal,dolphin))\n",
    "print(wordnet.path_similarity(homo,dolphin))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[Synset('entity.n.01'), Synset('physical_entity.n.01'), Synset('object.n.01'), Synset('whole.n.02'), Synset('living_thing.n.01'), Synset('organism.n.01'), Synset('animal.n.01'), Synset('chordate.n.01'), Synset('vertebrate.n.01'), Synset('mammal.n.01'), Synset('placental.n.01'), Synset('aquatic_mammal.n.01'), Synset('cetacean.n.01')]] \n",
      "\n",
      "\n",
      "[[Synset('entity.n.01'), Synset('physical_entity.n.01'), Synset('object.n.01'), Synset('whole.n.02'), Synset('living_thing.n.01'), Synset('organism.n.01'), Synset('animal.n.01'), Synset('chordate.n.01'), Synset('vertebrate.n.01'), Synset('mammal.n.01')]] \n",
      "\n",
      "\n",
      "[[Synset('entity.n.01'), Synset('physical_entity.n.01'), Synset('causal_agent.n.01'), Synset('person.n.01'), Synset('homosexual.n.01')], [Synset('entity.n.01'), Synset('physical_entity.n.01'), Synset('object.n.01'), Synset('whole.n.02'), Synset('living_thing.n.01'), Synset('organism.n.01'), Synset('person.n.01'), Synset('homosexual.n.01')]] \n",
      "\n",
      "\n",
      "[[Synset('entity.n.01'), Synset('physical_entity.n.01'), Synset('object.n.01'), Synset('whole.n.02'), Synset('living_thing.n.01'), Synset('organism.n.01'), Synset('animal.n.01'), Synset('chordate.n.01'), Synset('vertebrate.n.01'), Synset('aquatic_vertebrate.n.01'), Synset('fish.n.01'), Synset('bony_fish.n.01'), Synset('teleost_fish.n.01'), Synset('spiny-finned_fish.n.01'), Synset('percoid_fish.n.01'), Synset('dolphinfish.n.02')]] \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Exam 10.4\n",
    "cetacean = wordnet.synset('cetacean.n.01')\n",
    "mammal = wordnet.synset('mammal.n.01')\n",
    "homo = wordnet.synset('homo.n.01')\n",
    "dolphin = wordnet.synset('dolphin.n.01')\n",
    "print(cetacean.hypernym_paths(),'\\n\\n')\n",
    "print(mammal.hypernym_paths(),'\\n\\n')\n",
    "print(homo.hypernym_paths(),'\\n\\n')\n",
    "print(dolphin.hypernym_paths(),'\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ข้อ 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello:  2\n"
     ]
    }
   ],
   "source": [
    "#exam11\n",
    "from nltk.corpus import cmudict\n",
    "def syllable(word):\n",
    "    return len([1 for p in cmudict.dict()[word.lower()][0] if p[len(p)-1].isnumeric()])\n",
    "print('Hello: ', syllable('hello'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ข้อ 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "(1, 83834)\n",
      "1.5898596477882165\n"
     ]
    }
   ],
   "source": [
    "#exam12\n",
    "from nltk import FreqDist\n",
    "from nltk.corpus import inaugural\n",
    "from nltk.corpus import cmudict\n",
    "from collections import Counter\n",
    "\n",
    "# ไม่ใช้ฟังก์ชันจากข้อ 11 เพราะต้อง optimize ความเร็ว\n",
    "wordlist = [w.lower() for w in inaugural.words() if w.isalpha()]\n",
    "vocab = set(wordlist)\n",
    "my_syllable_dict = {}\n",
    "search_dict = cmudict.dict()\n",
    "for word in vocab:\n",
    "    if word in search_dict:\n",
    "        my_syllable_dict[word] = len([1 for p in search_dict[word][0] if p[len(p)-1].isnumeric()])\n",
    "count_list = []\n",
    "for word in wordlist:\n",
    "    if word in my_syllable_dict:\n",
    "        count_list.append(my_syllable_dict[word])\n",
    "#คำที่พยางยาวที่สุด\n",
    "print(max(count_list))\n",
    "#คำที่ถี่ที่สุด\n",
    "count_list_counter = Counter(count_list)\n",
    "print(count_list_counter.most_common(1)[0])\n",
    "#ค่าเฉลี่ยทั้งหมด\n",
    "print(sum(count_list) / float(len(count_list)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
