{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# สัปดาห์ที่ 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### คำถามก่อนเรียน\n",
    "- จัดการกับข้อความขนาดใหญ่อย่างไร\n",
    "- สกัด คำ/วลี ที่เป็นรูปแบบและเนื้อหาสำคัญของเอกสารอัตโนมัติอย่างไร\n",
    "- เราจะใช้ Python แก้อย่างไร\n",
    "- อะไรคือความท้าทายของ NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NOTE\n",
    "- Collocation (คำประสม: ผสมคำเดิมเป็นคำใหม่)\n",
    "- bi-gram พิจาณณาโทเคน 2 ตัวที่อยู่ติดกัน"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Introductory Examples for the NLTK Book ***\n",
      "Loading text1, ..., text9 and sent1, ..., sent9\n",
      "Type the name of the text or sentence to view it.\n",
      "Type: 'texts()' or 'sents()' to list the materials.\n",
      "text1: Moby Dick by Herman Melville 1851\n",
      "text2: Sense and Sensibility by Jane Austen 1811\n",
      "text3: The Book of Genesis\n",
      "text4: Inaugural Address Corpus\n",
      "text5: Chat Corpus\n",
      "text6: Monty Python and the Holy Grail\n",
      "text7: Wall Street Journal\n",
      "text8: Personals Corpus\n",
      "text9: The Man Who Was Thursday by G . K . Chesterton 1908\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.book import *\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Accessing', 'Text'), ('Text', 'corpora'), ('corpora', 'and'), ('and', 'Lexical'), ('Lexical', 'Resources')]\n"
     ]
    }
   ],
   "source": [
    "# ตัวอย่างการใช้งาน bigram\n",
    "text = \"Accessing Text corpora and Lexical Resources\"\n",
    "wordlist = text.split()\n",
    "bigram = nltk.bigrams(wordlist) #สร้างคำที่อยู่ใกล้กัน โดยจะได้ n-1 คู่\n",
    "print(list(bigram))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "## คำถาม: ใน text4 มีคำที่ยาวมากที่สุดมีความยาวมากที่สุด\n",
    "## คำถาม2: ความยาวที่มีความถี่มากที่สุด\n",
    "text4_length_list = [len(w) for w in text4]\n",
    "print(max(text4_length_list)) #คำตอบ1: คำยาวสุด (หา key เยอะสุดของ FreqDist(text4))\n",
    "text4_length_counter = Counter(text4_length_list)\n",
    "print(text4_length_counter.most_common(1)[0][0]) #คำตอบ2: ได้ถี่สูงสุด (FreqDist(text4).max()ก็ได้)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## หัวข้อที่ 2: Accessing Text Corpora and Lexical Resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### คำถาม\n",
    "- อะไรคือประโยชน์ของคลังข้อความ (text corpora) และทรัพยากรคำศัพท์ text resource **ตอบ** คลังคือรวมข้อความมาให้แล้ว :O "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## แสดงไฟล์ที่มีทั้งหมดใน gutenberg (https://www.gutenberg.org/)\n",
    "len(gutenberg.fileids())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "192427"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## ดึงคำทั้งหมดออกมาจากหนังสือ\n",
    "emma = gutenberg.words('austen-emma.txt')\n",
    "len(emma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7344"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# คำถาม: นับคำทั้งหมด ไม่สนใจเคส\n",
    "len(set([w.lower() for w in emma])) #ตีีกกาแล้ววน for เรียก list comprehension "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# แคส corpus ให้เป็น nltk.Text เพื่อให้ใช้คำสั่งของ nltk ได้ เช่น collocation, similar, concorden\n",
    "emma = nltk.Text(gutenberg.words('austen-emma.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.609909212324673\n",
      "114.4312435500516\n",
      "1.203496153605604\n"
     ]
    }
   ],
   "source": [
    "# คำถาม ความยาวเฉลี่ยของคำ, ความยาวเฉลี่ยประโยค จำนวนครั้งเฉลี่ยที่ถูกคำใช้\n",
    "# ประกาศตัวแปร\n",
    "emma_raw = gutenberg.raw('austen-emma.txt') #ตัวอักษรทั้งหมดแบบดิบ ไม่กรองตัวอักษร แท็บ บรรทัดออกเลย\n",
    "emma_words = gutenberg.words('austen-emma.txt') #คำทั้งหมด\n",
    "emma_sents = gutenberg.sents('austen-emma.txt') #ประโยคทั้งหมด\n",
    "# หาความยาวเฉลี่ยคำ\n",
    "print(len(emma_raw) / len(emma_words))\n",
    "# หาความยาวเฉลี่ยของประโยค\n",
    "print(len(emma_raw) / len(emma_sents))\n",
    "# จำนวนครั้งเฉลี่ยที่ถูกคำใช้ (lexical diversity)\n",
    "print(len(emma_words)  / len(set([w.lower for w in emma_words])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.604031209362809\n",
      "359.125\n",
      "2.5463576158940397\n"
     ]
    }
   ],
   "source": [
    "## ลองกับคลังอื่นดีกว่า\n",
    "washington_raw = inaugural.raw('1789-Washington.txt') #ตัวอักษรทั้งหมดแบบดิบ ไม่กรองตัวอักษร แท็บ บรรทัดออกเลย\n",
    "washington_words = inaugural.words('1789-Washington.txt') #คำทั้งหมด\n",
    "washington_sents = inaugural.sents('1789-Washington.txt') #ประโยคทั้งหมด\n",
    "# หาความยาวเฉลี่ยคำ\n",
    "print(len(washington_raw) / len(washington_words))\n",
    "# หาความยาวเฉลี่ยของประโยค\n",
    "print(len(washington_raw) / len(washington_sents))\n",
    "# จำนวนครั้งเฉลี่ยที่ถูกคำใช้ (lexical diversity)\n",
    "print(len(washington_words)  / len(set([w.lower() for w in washington_words])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "austen-emma.txt\t\t4.609909212324673\t\t114.4312435500516\t\t26.201933551198255\n",
      "austen-persuasion.txt\t\t4.749793727271801\t\t124.4440886042167\t\t16.82450728363325\n",
      "austen-sense.txt\t\t4.753785952421314\t\t134.63132626525305\t\t22.11088552241137\n",
      "bible-kjv.txt\t\t4.286881563819072\t\t143.92432647908845\t\t79.16143181640166\n",
      "blake-poems.txt\t\t4.567033756284415\t\t87.10730593607306\t\t5.442345276872964\n",
      "bryant-stories.txt\t\t4.489300433741879\t\t87.12504366049599\t\t14.10228426395939\n",
      "burgess-busterbrown.txt\t\t4.464641670621737\t\t80.32542694497154\t\t12.163566388710713\n",
      "carroll-alice.txt\t\t4.233216065669891\t\t84.78860833822665\t\t12.940060698027315\n",
      "chesterton-ball.txt\t\t4.716173862839705\t\t95.7208621050429\t\t11.637192561487703\n",
      "chesterton-brown.txt\t\t4.724783007796614\t\t106.83893851812927\t\t11.042211957916345\n",
      "chesterton-thursday.txt\t\t4.63099417739442\t\t85.65606627471941\t\t10.901401795558355\n",
      "edgeworth-parents.txt\t\t4.4391184023772565\t\t91.41329423264906\t\t24.939386764531786\n",
      "melville-moby_dick.txt\t\t4.76571875515204\t\t123.56993736951983\t\t15.136614241773549\n",
      "milton-paradise.txt\t\t4.835734572682675\t\t252.95515937331172\t\t10.73328899235118\n",
      "shakespeare-caesar.txt\t\t4.347539968257655\t\t51.92325473878872\t\t8.520118733509236\n",
      "shakespeare-hamlet.txt\t\t4.3597698072805136\t\t52.4407598197038\t\t7.921967769296014\n",
      "shakespeare-macbeth.txt\t\t4.336689714779602\t\t52.622443628736235\t\t6.680138568129331\n",
      "whitman-leaves.txt\t\t4.591950052620365\t\t167.34470588235294\t\t12.438403469322198\n"
     ]
    }
   ],
   "source": [
    "# เขียนตีตารางใน gutenberg ให้หมด\n",
    "for book_file in gutenberg.fileids():\n",
    "    raw = gutenberg.raw(book_file)\n",
    "    words = gutenberg.words(book_file)\n",
    "    sents = gutenberg.sents(book_file)\n",
    "    avg_word = len(raw) / len(words)\n",
    "    avg_sentent = len(raw) / len(sents)\n",
    "    lexical_density = len(words)  / len(set([w.lower() for w in words]))\n",
    "    print(book_file+\"\\t\\t\"+str(avg_word)+\"\\t\\t\"+str(avg_sentent)+\"\\t\\t\"+str(lexical_density))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['now', 'im', 'left', 'with', 'this', 'gay', 'name'], [':P'], ...]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## เนื่องจาก nps chat ต้อง parse XML เราจึงใช้ .posts เพื่อเข้าถึงข้อความแทน\n",
    "nps_chat.posts('10-19-20s_706posts.xml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['firefox.txt',\n",
       " 'grail.txt',\n",
       " 'overheard.txt',\n",
       " 'pirates.txt',\n",
       " 'singles.txt',\n",
       " 'wine.txt']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ตัวอย่างข้อความจากเว็บ\n",
    "webtext.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['adventure',\n",
       " 'belles_lettres',\n",
       " 'editorial',\n",
       " 'fiction',\n",
       " 'government',\n",
       " 'hobbies',\n",
       " 'humor',\n",
       " 'learned',\n",
       " 'lore',\n",
       " 'mystery',\n",
       " 'news',\n",
       " 'religion',\n",
       " 'reviews',\n",
       " 'romance',\n",
       " 'science_fiction']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# จากแหล่งหลากหลาย 500 แหล่ง สร้างในมหาลัย brown (https://en.wikipedia.org/wiki/Brown_Corpus)\n",
    "brown.categories() #แสดงประเภท"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Now', 'that', 'he', 'knew', 'himself', 'to', 'be', ...]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# แสดงเป็นประเภท categories\n",
    "brown.words(categories=['romance','science_fiction'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "can: 93\n",
      "could: 86\n",
      "may: 66\n",
      "might: 38\n",
      "must: 50\n",
      "will: 389\n"
     ]
    }
   ],
   "source": [
    "# ตรวจสอบการใช้คำ\n",
    "news_text = brown.words(categories='news')\n",
    "modal_words = ['can','could','may','might','must','will']\n",
    "word_freq = FreqDist(news_text)\n",
    "for modal in modal_words:\n",
    "    print(modal+\": \"+str(word_freq[modal]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## การบ้าน\n",
    "- ท้ายบท 2 ข้อ (1,2,3,15,19)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
